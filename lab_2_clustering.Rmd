---
title: "Lab 2 Clustering"
output:
  html_document:
    df_print: paged
---
## 0. Setup

Load packages
```{r setup}
library(tidyverse)
```

## 1. K-Means Clustering

Create simulated data in which there truly are two clusters.  Need to set a seed to ensure reproducible results since the first step of the k_means algorithm is to randomly assign each datapoint to a cluster.
```{r sim_data}
set.seed(2)
sim_data <- tibble("obs" = 1:50, "x1" = rnorm(50), "x2" = rnorm(50)) %>%
  mutate(x1 = if_else(obs <= 25, x1 + 3, x1 - 4))

sim_data %>%
  ggplot(aes(x1, x2)) +
  geom_point()
```

Do K-means clustering with K = 2, ie divide the data into two clusters.  Colour each point based on the cluster assigned by k-means, and plot the mean of each cluster.

'centers' specifies the number of clusters to divide the data into.

'nstart' specifies the number of different initial random assignments to run the k_means algorithm for. If nstart is small (eg 1 or 2), then it is more likely that a local minimum of the within-cluster sum-of-squares will be found.
The output from k_means is the run of k_means with the initial random assignment which gives the minimum within-cluster sum-of-squares.

```{r 2_means}
km_out <- sim_data %>%
  select(-obs) %>%
  kmeans(centers = 2, nstart = 20)

km_out

sim_data_out <- sim_data %>%
  mutate(cluster_k2 = as.character(km_out$cluster))

cluster_means <- sim_data_out %>%
  group_by(cluster_k2) %>%
  summarise(x1_mean = mean(x1), x2_mean = mean(x2))

ggplot(sim_data_out, aes(x1, x2, colour = cluster_k2)) +
  geom_point() +
  geom_point(data = cluster_means, aes(x1_mean, x2_mean), size = 5, shape = 4)
```

As above, but with k = 3, ie divide the data into 3 clusters.
```{r 3_means}
km_out <- sim_data %>%
  select(-obs) %>%
  kmeans(centers = 3, nstart = 20)

km_out

sim_data_out <- sim_data %>%
  mutate(cluster_k3 = as.character(km_out$cluster))

cluster_means <- sim_data_out %>%
  group_by(cluster_k3) %>%
  summarise(x1_mean = mean(x1), x2_mean = mean(x2))

ggplot(sim_data_out, aes(x1, x2, colour = cluster_k3)) +
  geom_point() +
  geom_point(data = cluster_means, aes(x1_mean, x2_mean), size = 5, shape = 4)
```

Investigate effect of varying nstart an centers parameters on the clusters each point is assigned to.  

Create a grid of different values for centers (k) and nstart. Apply kmeans to simdata for each pair of values for centers and nstart.

```{r var_params}
grid <- expand.grid(
  k = seq(from = 2L, to = 3L, by = 1L),
  nstart = seq(from = 1L, by = 10L, length.out = 3L)
  ) %>%
  as_tibble() 

grid <- grid %>%
  mutate(data = list(sim_data),
         km_out = purrr::pmap(list("x" = data, "centers" = k, "nstart" = nstart), kmeans)
         )

grid
```

Add cluster variable to data for each run of kmeans with different values for k and nstart.
```{r assign_cluster}
# Function to extract vector from kmeans which gives cluster each observation assigned to.
extr_cluster <- function(mod)
  tibble("cluster" = as.character(mod$cluster))

# Function to extract value of total within-cluster sum-of-squares
extr_wcss <- function(mod)
  mod$tot.withinss

grid <- grid %>%
  mutate(cluster = purrr::map(km_out, extr_cluster),
         data = purrr::map2(data, cluster, bind_cols),
         tot.withinss = purrr::map_dbl(km_out, extr_wcss)
        )

grid
```

Plot chart showing clusters observations assigned to for each value of k and nstart.  Chart shows that iwith this data the result of kmeans not sensitive to value of nstart, nstart = 1 produces the same results. k = 2 provides better classification than k = 3, as expected given the simulated data.
```{r chart_k_nstart}
chart_data <- grid %>%
  select(k, nstart, data) %>%
  unnest()

chart_data %>%
  ggplot(aes(x = x1, y = x2, colour = cluster)) +
  geom_point() +
  facet_grid(nstart ~ k)
  
```

## 1. Hierarchical Clustering
