---
title: "Exercises"
output:
  html_document:
    df_print: paged
---

## 0. Setup

Load packages and set seed.
```{r message = FALSE}

library(ISLR)
library(tidyverse)
library(plotly)

set.seed(24)
```

## Exercises
### Ex 7

The correlation-based distance and the squared Euclidean distance are proportional to each other if each observation has been scaled to have mean 0 and standard deviation 1.  Show that this is true for the USArrests data.  Correlation based distance is 1 - cor, so observations which are highly positively correlated (cor ~ 1), will have distance close to zero.

```{r load_usarrests}
usarr_df <- as_tibble(USArrests)
states <- row.names(USArrests)
```

Scale each observation, not each feature, to have mean 0 and standard deviation 1.  This is why we have to take the transpose before scaling. Plotting 1 - correlation of each observation against the Euclidean distance between each observation gives a straight line, ie they are proportional to each other.
```{r ex_7}
usarr_obs_scaled <- t(scale(t(usarr_df)))

usarr_sc_euc_dist2 <- as.matrix(dist(usarr_obs_scaled) ^ 2)

usarr_sc_corr_dist <- 1 - cor(t(usarr_obs_scaled)) 

x <- usarr_sc_corr_dist[lower.tri(usarr_sc_corr_dist)]
y <- usarr_sc_euc_dist2[lower.tri(usarr_sc_euc_dist2)]

plot(x, y, xlab = "1 - cor", ylab = "Euclidean distance")
```

### Ex 8

Calculate proportion of variance explained by each principal component in two ways.
1. Using output from prcomp function.
2. Directly from the principal component loadings.

```{r ex_8_1}
pr_out <- prcomp(usarr_df, scale = TRUE)

pr_var <- pr_out$sdev ^ 2
pve <- pr_var / sum(pr_var)

pve
```

Calculating the pve directly from the principal component loadings, and expressing each observation in terms of principal components, and calculatng the variance across the observations for each principal component gives the same results as above.  Note that because the variables have been scaled, the variance of a principal component can be calculated as the sum of the observation values squared for each principal component.
```{r ex_8_2}
pr_load <- pr_out$rotation

usarr_var_scaled <- scale(usarr_df)

pr_var2 <- as_tibble(as.matrix(usarr_var_scaled) %*% pr_load) %>%
  mutate_all(funs(.^2)) %>%
  summarise_all(sum)

pve2 <- pr_var2 / sum(pr_var2)

pve2
```

### Ex 9

Compare the effect of scaling variables on the clusters assigned by hierarchical clustering.
```{r ex_9_no_scale}
hc_noscale <- hclust(dist(usarr_df), method = "complete")

plot(hc_noscale, xlab = "", sub = "", labels = row.names(USArrests))

usarr_noscale <- usarr_df 
usarr_noscale$cluster <- as.character(cutree(hc_noscale, 3))
usarr_noscale$state <- row.names(USArrests)
usarr_noscale$scaled <- "no"

usarr_noscale[1:10, ]
```

```{r ex_9_scale}
hc_scale <- hclust(dist(scale(usarr_df)), method = "complete")

plot(hc_scale, xlab = "", sub = "", labels = row.names(USArrests))

usarr_scale <- usarr_df 
usarr_scale$cluster <- as.character(cutree(hc_scale, 3)) 
usarr_scale$state <- row.names(USArrests)
usarr_scale$scaled <- "yes"

usarr_scale[1:10, ]
```

The charts below compare the clusters using scaling and no scaling.  

The first chart shows each state by Assault and UrbanPop. With no scaling the clusters are assigned almost entirely by the Assault variable, which has much larger values than the other variables without scaling, so this variable dominates the Euclidean distance of points within clusters which hierarchical clustering aims to minimise.  With scaling, UrbabPop is also taken into account to assign stats to clusters.

Looking at charts with 2 variables other than Assault, the clustering with no scaling has not identified clusters within these variables very well at all compared to clustering with scaling.

The variables should be scaled before clustering. This is because:
* The crime rates and urban population are in different units.  Crime rates are arrests per 100,000, UrbanPop is a % of the population in the state that is urban.
* Murder and Rape are more serious crimes than Assault.  Assault occurs much more often and so it dominates wthe clustering analysis without scalng, but we don't want it to dominate at the expense of ignoring murder and rape.

```{r ex_9_compare}
usarr_hc <- dplyr::bind_rows(usarr_noscale, usarr_scale) 

usarr_hc %>%
  ggplot(aes(Assault, UrbanPop, colour = cluster)) +
  geom_point() +
  geom_vline(xintercept = 130, colour = "grey") +
  geom_vline(xintercept = 220, colour = "grey") +
  geom_hline(yintercept = 66, colour = "grey") +
  facet_wrap(~ scaled)

usarr_hc %>%
  ggplot(aes(Assault, Rape, colour = cluster)) +
  geom_point() +
  geom_vline(xintercept = 130, colour = "grey") +
  geom_vline(xintercept = 220, colour = "grey") +
  geom_hline(yintercept = 26, colour = "grey") +
  facet_wrap(~ scaled)

usarr_hc %>%
  ggplot(aes(Assault, Murder, colour = cluster)) +
  geom_point() +
  geom_vline(xintercept = 130, colour = "grey") +
  geom_vline(xintercept = 220, colour = "grey") +
  geom_hline(yintercept = 12.5, colour = "grey") +
  facet_wrap(~ scaled)

usarr_hc %>%
  ggplot(aes(Murder, Rape, colour = cluster)) +
  geom_point() +
  facet_wrap(~ scaled)

usarr_hc %>%
  ggplot(aes(UrbanPop, Rape, colour = cluster)) +
  geom_point() +
  facet_wrap(~ scaled)

```

### Ex 10

Generate simulated data and then perform PCA and k-means clustering.

First simulate 60 observations with 50 variables with 3 classes (ie clusters).  Shift the mean of observations in different clases so that they have significant different variable values for each of the 50 variables.

```{r sim_data}
set.seed(25)

var_means <- as_tibble(matrix(sample(1:100, 150, replace = TRUE), nrow = 3, ncol = 50), .name_repair = NULL)
var_means$cluster <- c("a", "b", "c")

var_means <- var_means %>%
  select(cluster, everything()) 

sim_rnorm <- function(x){
    map_dbl(x, rnorm, n = 1, sd = 60)
}

sim_data <- tibble("cluster" = rep(c("a", "b","c"), each = 20)) %>%
  left_join(var_means, by = "cluster") %>%
  mutate_if(is.numeric, sim_rnorm)
```

Do PCA on simulated data.  Plot the observations for first two variables, and do another plot with the first 2 principal components.  The plot with the raw variables shows overlap between the clusters, the plot with the first 2 principal components shows separation.
```{r pca}
pr_out_10 <- sim_data %>%
  select_if(is.numeric) %>%
  prcomp(scale = TRUE)

pca_sim_data <- as_tibble(pr_out_10$x)
pca_sim_data$cluster <- sim_data$cluster

sim_data %>%
  ggplot(aes(V1, V2, colour = cluster)) +
  geom_point()

pca_sim_data %>%
  ggplot(aes(PC1, PC2, colour = cluster)) +
  geom_point()

p_var <- pr_out_10$sdev^2
plot(p_var / sum(p_var), xlab = "Principal component", ylab = "Proportion of variance explained")
```

Carry out k-means clustering with k = 2, 3, 4 on the raw observations (unscaled, actual variables not principal components).
```{r kmeans}
set.seed(25)

# Run kmeans for each value of k rom 2 to 4.
kmeans_res <- tibble("k" = 2L:4L) %>%
  mutate(data = list(sim_data),
         data_numeric = list(select_if(sim_data, is.numeric)),
         km_out = purrr::pmap(list("x" = data_numeric, "centers" = k), kmeans, nstart = 30)
         )

# Function to extract vector from kmeans which gives cluster each observation assigned to.
extr_cluster <- function(mod)
  tibble("cluster_kmeans" = as.character(mod$cluster))

kmeans_res <- kmeans_res %>%
  mutate(cluster_kmeans = purrr::map(km_out, extr_cluster),
         data = purrr::map2(data, cluster_kmeans, bind_cols)
        ) %>%
  select(k, data) %>%
  unnest() %>%
  select(k, cluster, cluster_kmeans, everything())
```

Summarise how well k-means has separated the data into clusters for each value of k.  

For k = 2, all of the observations from cluster a were assigned to the same cluster (1) by kmeans.  One observation from b was assigned to the same cluster as a.  All of the othe observations from clusters b and c were assigned to another cluster (2).

For k = 3, kmeans assigned each observation from a to the same cluster (3), b to 2 and c to 1, ie it assigned the clusters correctly.

For k = 4, as per k = 3, but observations from cluster b have been split into two clusters with 6 in one cluster and 14 in another.
```{r assess_kmeans}
kmeans_res %>%
  group_by(k, cluster, cluster_kmeans) %>%
  summarise(n_obs = n())
```

Carry out k-means clustering with k = 3 on the first two principal component vectors. The observations are assigned to the correct clusters, this suggests that the information contained within the first two principal components is sufficient to separate the observations into the correct clusters, and the other principal components can be ignored.

```{r kmeans_pca}
set.seed(25)

pca_sim_data_2 <- select(pca_sim_data, 1:2)
pca_sim_data_2$cluster <- sim_data$cluster

# Run kmeans for each value of k rom 2 to 4.
kmeans_res_pc2 <- tibble("k" = 3L) %>%
  mutate(data = list(pca_sim_data_2),
         data_numeric = list(select_if(pca_sim_data_2, is.numeric)),
         km_out = purrr::pmap(list("x" = data_numeric, "centers" = k), kmeans, nstart = 30)
         )

# Function to extract vector from kmeans which gives cluster each observation assigned to.
extr_cluster <- function(mod)
  tibble("cluster_kmeans" = as.character(mod$cluster))

kmeans_res_pc2 <- kmeans_res_pc2 %>%
  mutate(cluster_kmeans = purrr::map(km_out, extr_cluster),
         data = purrr::map2(data, cluster_kmeans, bind_cols)
        ) %>%
  select(k, data) %>%
  unnest() %>%
  select(k, cluster, cluster_kmeans, everything())

kmeans_res_pc2 %>%
  group_by(k, cluster, cluster_kmeans) %>%
  summarise(n_obs = n())
```

