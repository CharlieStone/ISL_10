---
title: "Exercises"
output:
  html_document:
    df_print: paged
---

## 0. Setup

Load packages and set seed.
```{r message = FALSE}

library(ISLR)
library(tidyverse)
library(plotly)

set.seed(24)
```

## Exercises
### Ex 7

The correlation-based distance and the squared Euclidean distance are proportional to each other if each observation has been scaled to have mean 0 and standard deviation 1.  Show that this is true for the USArrests data.  Correlation based distance is 1 - cor, so observations which are highly positively correlated (cor ~ 1), will have distance close to zero.

```{r load_usarrests}
usarr_df <- as_tibble(USArrests)
states <- row.names(USArrests)
```

Scale each observation, not each feature, to have mean 0 and standard deviation 1.  This is why we have to take the transpose before scaling. Plotting 1 - correlation of each observation against the Euclidean distance between each observation gives a straight line, ie they are proportional to each other.
```{r ex_7}
usarr_obs_scaled <- t(scale(t(usarr_df)))

usarr_sc_euc_dist2 <- as.matrix(dist(usarr_obs_scaled) ^ 2)

usarr_sc_corr_dist <- 1 - cor(t(usarr_obs_scaled)) 

x <- usarr_sc_corr_dist[lower.tri(usarr_sc_corr_dist)]
y <- usarr_sc_euc_dist2[lower.tri(usarr_sc_euc_dist2)]

plot(x, y, xlab = "1 - cor", ylab = "Euclidean distance")
```

### Ex 8

Calculate proportion of variance explained by each principal component in two ways.
1. Using output from prcomp function.
2. Directly from the principal component loadings.

```{r ex_8_1}
pr_out <- prcomp(usarr_df, scale = TRUE)

pr_var <- pr_out$sdev ^ 2
pve <- pr_var / sum(pr_var)

pve
```

Calculating the pve directly from the principal component loadings, and expressing each observation in terms of principal components, and calculatng the variance across the observations for each principal component gives the same results as above.  Note that because the variables have been scaled, the variance of a principal component can be calculated as the sum of the observation values squared for each principal component.
```{r ex_8_2}
pr_load <- pr_out$rotation

usarr_var_scaled <- scale(usarr_df)

pr_var2 <- as_tibble(as.matrix(usarr_var_scaled) %*% pr_load) %>%
  mutate_all(funs(.^2)) %>%
  summarise_all(sum)

pve2 <- pr_var2 / sum(pr_var2)

pve2
```

### Ex 9

```{r ex_9_no_scale}
hc_noscale <- hclust(dist(usarr_df), method = "complete")

plot(hc_noscale, xlab = "", sub = "", labels = row.names(USArrests))

usarr_noscale <- usarr_df 
usarr_noscale$cluster <- as.character(cutree(hc_noscale, 3))
usarr_noscale$state <- row.names(USArrests)
usarr_noscale$scaled <- "no"

usarr_noscale[1:10, ]
```

```{r ex_9_scale}
hc_scale <- hclust(dist(scale(usarr_df)), method = "complete")

plot(hc_scale, xlab = "", sub = "", labels = row.names(USArrests))

usarr_scale <- usarr_df 
usarr_scale$cluster <- as.character(cutree(hc_scale, 3)) 
usarr_scale$state <- row.names(USArrests)
usarr_scale$scaled <- "yes"

usarr_scale[1:10, ]
```

The charts below compare the clusters using scaling and no scaling.  

The first chart shows each state by Assault and UrbanPop. With no scaling the clusters are assigned almost entirely by the Assault variable, which has much larger values than the other variables without scaling, so this variable dominates the Euclidean distance of points within clusters which hierarchical clustering aims to minimise.  With scaling, UrbabPop is also taken into account to assign stats to clusters.

Looking at charts with 2 variables other than Assault, the clustering with no scaling has not identified clusters within these variables very well at all compared to clustering with scaling.

The variables should be scaled before clustering. This is because:
* The crime rates and urban population are in different units.  Crime rates are arrests per 100,000, UrbanPop is a % of the population in the state that is urban.
* Murder and Rape are more serious crimes than Assault.  Assault occurs much more often and so it dominates wthe clustering analysis without scalng, but we don't want it to dominate at the expense of ignoring murder and rape.

```{r ex_9_compare}
usarr_hc <- dplyr::bind_rows(usarr_noscale, usarr_scale) 

usarr_hc %>%
  ggplot(aes(Assault, UrbanPop, colour = cluster)) +
  geom_point() +
  geom_vline(xintercept = 130, colour = "grey") +
  geom_vline(xintercept = 220, colour = "grey") +
  geom_hline(yintercept = 66, colour = "grey") +
  facet_wrap(~ scaled)

usarr_hc %>%
  ggplot(aes(Assault, Rape, colour = cluster)) +
  geom_point() +
  geom_vline(xintercept = 130, colour = "grey") +
  geom_vline(xintercept = 220, colour = "grey") +
  geom_hline(yintercept = 26, colour = "grey") +
  facet_wrap(~ scaled)

usarr_hc %>%
  ggplot(aes(Assault, Murder, colour = cluster)) +
  geom_point() +
  geom_vline(xintercept = 130, colour = "grey") +
  geom_vline(xintercept = 220, colour = "grey") +
  geom_hline(yintercept = 12.5, colour = "grey") +
  facet_wrap(~ scaled)

usarr_hc %>%
  ggplot(aes(Murder, Rape, colour = cluster)) +
  geom_point() +
  facet_wrap(~ scaled)

usarr_hc %>%
  ggplot(aes(UrbanPop, Rape, colour = cluster)) +
  geom_point() +
  facet_wrap(~ scaled)

```

