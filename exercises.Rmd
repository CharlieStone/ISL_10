---
title: "Exercises"
output:
  html_document:
    df_print: paged
---

## 0. Setup

Load packages and set seed.
```{r message = FALSE}

library(ISLR)
library(tidyverse)
library(plotly)

set.seed(24)
```

## Exercises
### Ex 7

The correlation-based distance and the squared Euclidean distance are proportional to each other if each observation has been scaled to have mean 0 and standard deviation 1.  Show that this is true for the USArrests data.  Correlation based distance is 1 - cor, so observations which are highly positively correlated (cor ~ 1), will have distance close to zero.

```{r load_usarrests}
usarr_df <- as_tibble(USArrests)
states <- row.names(USArrests)
```

Scale each observation, not each feature, to have mean 0 and standard deviation 1.  This is why we have to take the transpose before scaling. Plotting 1 - correlation of each observation against the Euclidean distance between each observation gives a straight line, ie they are proportional to each other.
```{r ex_7}
usarr_obs_scaled <- t(scale(t(usarr_df)))

usarr_sc_euc_dist2 <- as.matrix(dist(usarr_obs_scaled) ^ 2)

usarr_sc_corr_dist <- 1 - cor(t(usarr_obs_scaled)) 

x <- usarr_sc_corr_dist[lower.tri(usarr_sc_corr_dist)]
y <- usarr_sc_euc_dist2[lower.tri(usarr_sc_euc_dist2)]

plot(x, y, xlab = "1 - cor", ylab = "Euclidean distance")
```

### Ex 8

Calculate proportion of variance explained by each principal component in two ways.
1. Using output from prcomp function.
2. Directly from the principal component loadings.

```{r ex_8_1}
pr_out <- prcomp(usarr_df, scale = TRUE)

pr_var <- pr_out$sdev ^ 2
pve <- pr_var / sum(pr_var)

pve
```

Calculating the pve directly from the principal component loadings, and expressing each observation in terms of principal components, and calculatng the variance across the observations for each principal component gives the same results as above.  Note that because the variables have been scaled, the variance of a principal component can be calculated as the sum of the observation values squared for each principal component.
```{r ex_8_2}
pr_load <- pr_out$rotation

usarr_var_scaled <- scale(usarr_df)

pr_var2 <- as_tibble(as.matrix(usarr_var_scaled) %*% pr_load) %>%
  mutate_all(funs(.^2)) %>%
  summarise_all(sum)

pve2 <- pr_var2 / sum(pr_var2)

pve2
```

### Ex 9

Compare the effect of scaling variables on the clusters assigned by hierarchical clustering.
```{r ex_9_no_scale}
hc_noscale <- hclust(dist(usarr_df), method = "complete")

plot(hc_noscale, xlab = "", sub = "", labels = row.names(USArrests))

usarr_noscale <- usarr_df 
usarr_noscale$cluster <- as.character(cutree(hc_noscale, 3))
usarr_noscale$state <- row.names(USArrests)
usarr_noscale$scaled <- "no"

usarr_noscale[1:10, ]
```

```{r ex_9_scale}
hc_scale <- hclust(dist(scale(usarr_df)), method = "complete")

plot(hc_scale, xlab = "", sub = "", labels = row.names(USArrests))

usarr_scale <- usarr_df 
usarr_scale$cluster <- as.character(cutree(hc_scale, 3)) 
usarr_scale$state <- row.names(USArrests)
usarr_scale$scaled <- "yes"

usarr_scale[1:10, ]
```

The charts below compare the clusters using scaling and no scaling.  

The first chart shows each state by Assault and UrbanPop. With no scaling the clusters are assigned almost entirely by the Assault variable, which has much larger values than the other variables without scaling, so this variable dominates the Euclidean distance of points within clusters which hierarchical clustering aims to minimise.  With scaling, UrbabPop is also taken into account to assign stats to clusters.

Looking at charts with 2 variables other than Assault, the clustering with no scaling has not identified clusters within these variables very well at all compared to clustering with scaling.

The variables should be scaled before clustering. This is because:
* The crime rates and urban population are in different units.  Crime rates are arrests per 100,000, UrbanPop is a % of the population in the state that is urban.
* Murder and Rape are more serious crimes than Assault.  Assault occurs much more often and so it dominates wthe clustering analysis without scalng, but we don't want it to dominate at the expense of ignoring murder and rape.

```{r ex_9_compare}
usarr_hc <- dplyr::bind_rows(usarr_noscale, usarr_scale) 

usarr_hc %>%
  ggplot(aes(Assault, UrbanPop, colour = cluster)) +
  geom_point() +
  geom_vline(xintercept = 130, colour = "grey") +
  geom_vline(xintercept = 220, colour = "grey") +
  geom_hline(yintercept = 66, colour = "grey") +
  facet_wrap(~ scaled)

usarr_hc %>%
  ggplot(aes(Assault, Rape, colour = cluster)) +
  geom_point() +
  geom_vline(xintercept = 130, colour = "grey") +
  geom_vline(xintercept = 220, colour = "grey") +
  geom_hline(yintercept = 26, colour = "grey") +
  facet_wrap(~ scaled)

usarr_hc %>%
  ggplot(aes(Assault, Murder, colour = cluster)) +
  geom_point() +
  geom_vline(xintercept = 130, colour = "grey") +
  geom_vline(xintercept = 220, colour = "grey") +
  geom_hline(yintercept = 12.5, colour = "grey") +
  facet_wrap(~ scaled)

usarr_hc %>%
  ggplot(aes(Murder, Rape, colour = cluster)) +
  geom_point() +
  facet_wrap(~ scaled)

usarr_hc %>%
  ggplot(aes(UrbanPop, Rape, colour = cluster)) +
  geom_point() +
  facet_wrap(~ scaled)

```

### Ex 10

Generate simulated data and then perform PCA and k-means clustering.

First simulate 60 observations with 50 variables with 3 classes (ie clusters).  Shift the mean of observations in different clases so that they have significant different variable values for each of the 50 variables.

```{r sim_data}
set.seed(25)

var_means <- as_tibble(matrix(sample(1:100, 150, replace = TRUE), nrow = 3, ncol = 50), .name_repair = NULL)
var_means$cluster <- c("a", "b", "c")

var_means <- var_means %>%
  select(cluster, everything()) 

sim_rnorm <- function(x){
    map_dbl(x, rnorm, n = 1, sd = 60)
}

sim_data <- tibble("cluster" = rep(c("a", "b","c"), each = 20)) %>%
  left_join(var_means, by = "cluster") %>%
  mutate_if(is.numeric, sim_rnorm)
```

Do PCA on simulated data.  Plot the observations for first two variables, and do another plot with the first 2 principal components.  The plot with the raw variables shows overlap between the clusters, the plot with the first 2 principal components shows separation.
```{r pca}
pr_out_10 <- sim_data %>%
  select_if(is.numeric) %>%
  prcomp(scale = TRUE)

pca_sim_data <- as_tibble(pr_out_10$x)
pca_sim_data$cluster <- sim_data$cluster

sim_data %>%
  ggplot(aes(V1, V2, colour = cluster)) +
  geom_point()

pca_sim_data %>%
  ggplot(aes(PC1, PC2, colour = cluster)) +
  geom_point()

p_var <- pr_out_10$sdev^2
plot(p_var / sum(p_var), xlab = "Principal component", ylab = "Proportion of variance explained")
```

Carry out k-means clustering with k = 2, 3, 4 on the raw observations (unscaled, actual variables not principal components).
```{r kmeans}
set.seed(25)

# Run kmeans for each value of k rom 2 to 4.
kmeans_res <- tibble("k" = 2L:4L) %>%
  mutate(data = list(sim_data),
         data_numeric = list(select_if(sim_data, is.numeric)),
         km_out = purrr::pmap(list("x" = data_numeric, "centers" = k), kmeans, nstart = 30)
         )

# Function to extract vector from kmeans which gives cluster each observation assigned to.
extr_cluster <- function(mod)
  tibble("cluster_kmeans" = as.character(mod$cluster))

kmeans_res <- kmeans_res %>%
  mutate(cluster_kmeans = purrr::map(km_out, extr_cluster),
         data = purrr::map2(data, cluster_kmeans, bind_cols)
        ) %>%
  select(k, data) %>%
  unnest() %>%
  select(k, cluster, cluster_kmeans, everything())
```

Summarise how well k-means has separated the data into clusters for each value of k.  

For k = 2, all of the observations from cluster a were assigned to the same cluster (1) by kmeans.  One observation from b was assigned to the same cluster as a.  All of the othe observations from clusters b and c were assigned to another cluster (2).

For k = 3, kmeans assigned each observation from a to the same cluster (3), b to 2 and c to 1, ie it assigned the clusters correctly.

For k = 4, as per k = 3, but observations from cluster b have been split into two clusters with 6 in one cluster and 14 in another.
```{r assess_kmeans}
kmeans_res %>%
  group_by(k, cluster, cluster_kmeans) %>%
  summarise(n_obs = n())
```

Carry out k-means clustering with k = 3 on the first two principal component vectors. The observations are assigned to the correct clusters, this suggests that the information contained within the first two principal components is sufficient to separate the observations into the correct clusters, and the other principal components can be ignored.

```{r kmeans_pca}
set.seed(25)

pca_sim_data_2 <- select(pca_sim_data, 1:2)
pca_sim_data_2$cluster <- sim_data$cluster

# Run kmeans for each value of k rom 2 to 4.
kmeans_res_pc2 <- tibble("k" = 3L) %>%
  mutate(data = list(pca_sim_data_2),
         data_numeric = list(select_if(pca_sim_data_2, is.numeric)),
         km_out = purrr::pmap(list("x" = data_numeric, "centers" = k), kmeans, nstart = 30)
         )

# Function to extract vector from kmeans which gives cluster each observation assigned to.
extr_cluster <- function(mod)
  tibble("cluster_kmeans" = as.character(mod$cluster))

kmeans_res_pc2 <- kmeans_res_pc2 %>%
  mutate(cluster_kmeans = purrr::map(km_out, extr_cluster),
         data = purrr::map2(data, cluster_kmeans, bind_cols)
        ) %>%
  select(k, data) %>%
  unnest() %>%
  select(k, cluster, cluster_kmeans, everything())

kmeans_res_pc2 %>%
  group_by(k, cluster, cluster_kmeans) %>%
  summarise(n_obs = n()) 
```

### Ex 11

Load gene expressions data from www.statlearning.com. Book states that first 20 tissue samples are from healthy patients, the next 20 are from diseased patients.

Plot gene expression levels for first few variables.  Cannot see that these gene expression levels obviously split the data into clusters.  Not practical to continue to do this for 1000 variables!
```{r load_gene_data}
load_df <- read_csv("Ch10Ex11.csv", col_names = F) 
gene_df <- as_tibble(t(load_df))
gene_df$health <- rep(c("healthy", "diseased"), each = 20)

gene_df <- select(gene_df, health, everything())

# Scatter plots of observations for first few variables
ggplot(gene_df, aes(V1, V2, colour = health)) +
  geom_point()

ggplot(gene_df, aes(V1, V3, colour = health)) +
  geom_point()

ggplot(gene_df, aes(V2, V3, colour = health)) +
  geom_point()
```

Do hierarchical clustering on the gene expression data, to see if the gene expressions separate the tissue samples into two separate groups.  Look at using single, complete and average linkage methods.  Using correlation based distance as specified in the question.

For each type of linkage the diseased samples are at a similar height to each other and the healthy samples have a much higher height.  This means that the diseased samples typically have much more similar gene expression levels than the healthy samples. However, if were to cut tree at 2, then for complete linkage would get: 1 cluster with all healthy samples, and another with some healthy samples and some diseased.  Would need to cut tree at height of 0.8 to get all diseased samples in one cluster, and then each healthy sample would be in it's own cluster.

Results for other linkages are similar.  With tree cut at 2 clusters single linkage would give 1 cluster with one healthy sample, and another cluster containing all of the other samples.

```{r gene_hc}
calc_cor_dist <- function(df){
  as.dist(1 - cor(t(df))) 
}

gene_hc_df <- tibble("method" = c("complete", "average", "single")) %>%
  mutate(data_num = list(select_if(gene_df, is.numeric)),
         data = list(gene_df),
         cor_dist = purrr::map(data_num, calc_cor_dist),
         hc = purrr::map2(cor_dist, method, hclust))

plot_dgram <- function(hc, method_nm){
  plot(hc, main = method_nm, xlab ="", sub = "", cex = .9, labels = gene_df$health)
}

gene_hc_df <- gene_hc_df %>%
  mutate(dgram = purrr::map2(hc, method, plot_dgram)) %>%
  select(-dgram)
```

Plotting the first 20 gene expressions reveals little difference between genes 1 to 10, but substantial differences between 11 to 20.  

```{r gene_eda}
gene_df %>%
  select(1:21) %>%
  gather(V1:V20, key = "gene", value = "expression") %>%
  ggplot(aes(gene, expression, colour = health)) +
  geom_boxplot()
```

Looking at genes 21 to 39, no obvious difference.  Not practical to do boxplots for all 1000 genes.
```{r gene_eda_2}
gene_df %>%
  select(1, 22:40) %>%
  gather(V21:V39, key = "gene", value = "expression") %>%
  ggplot(aes(gene, expression, colour = health)) +
  geom_boxplot()
```

Plot 2d density diagram to see if any obvious difference in the 1000 gene expressions of healthy and diseased tissue samples, and to check whether the gene expressions have similar distriutions (ie whether need to be scaled before applying pca). From this chart we can clearly see that the gene expressions for genes 500 and 600 are clearly different between the healthy and diseased tissue samples.  It looks like this is simulated data with an artificial offset of + 2 added to the gene expressions for diseased tissue samples between 11 and 20 and from 500 to 600.  

It is still worth doing PCA to check wheher there are any other genes that vary signifiantly between the healthy and diseased samples which aren't obvious from the plot below. It is only obvious from the chart below beccause the offset has been applied to genes in the same block.  If had been applied randomly across the 1000, less likely to see in the chart below.  Although could have ordered by those with the biggest difference between the mean values of each gene between the two tissue samples and then plotted x axis with this ordering.  Anyway, should be able to see if similar results with PCA.

```{r plot_2d_density}
gene_df %>%
  gather(V1:V1000, key = "gene", value = "expression") %>%
  mutate(gene = as.numeric(str_replace(gene, "[:alpha:]", ""))) %>%
  ggplot(aes(gene, expression)) +
  geom_bin2d(binwidth = c(5, 0.5)) +
  facet_wrap(~ health)
```

Carry out PCA on unscaled genes to start with, then do scaling and see if this effects the results.  From plotting the PVE chart, it looks like the first two principal components explain most of the variation in the gene expression levels.  Plot observations with these two variables as the x and y axes to see if separates the data into clusters.

We see that the first principal component clearly splits the samples into healthy and diseased.

```{r gene_pca_unscaled}
pr_gene_unscaled <- prcomp(select_if(gene_df, is.numeric), scale = FALSE)

p_var <- pr_gene_unscaled$sdev^2
pve <- p_var/sum(p_var)
plot(pve)

gene_df_z <- as_tibble(pr_gene_unscaled$x)
gene_df_z$health <- gene_df$health

gene_df_z %>%
  ggplot(aes(PC1, PC2, colour = health)) +
  geom_point()
```

Now investigate to see which genes are given the highest loadings in the first principal component.  Perhaps unsurpisingly, it is the genes between 500 and 600, which were the genes with higher mean and standard deviation, and so without scaling, there will naturally be more variation along these genes.

```{r gene_pca_unscaled_1}
pr_rotation <- as_tibble(pr_gene_unscaled$rotation)
pr_rotation$gene <- row.names(pr_gene_unscaled$rotation)
pr_rotation <- pr_rotation %>%
  select(gene, everything()) %>%
  arrange(-abs(PC1))

slice(pr_rotation, 1:10)
```

Scatterplots with axes as genes with top loadings in the first principal component.  Can see that these genes split the observations into healthy and diseased.  This is in contrast to the plots at the beginning of this section for V1, V2 and V3.
```{r gene_pc_plot}
ggplot(gene_df, aes(V600, V584, colour = health)) +
  geom_point()

ggplot(gene_df, aes(V549, V540, colour = health)) +
  geom_point()

ggplot(gene_df, aes(V502, V582, colour = health)) +
  geom_point()
```

Redo analysis but with scaled data. 

```{r gene_pca_scaled}
pr_gene_unscaled <- prcomp(select_if(gene_df, is.numeric), scale = TRUE)

p_var <- pr_gene_unscaled$sdev^2
pve <- p_var/sum(p_var)
plot(pve)

gene_df_z <- as_tibble(pr_gene_unscaled$x)
gene_df_z$health <- gene_df$health

gene_df_z %>%
  ggplot(aes(PC1, PC2, colour = health)) +
  geom_point()
```

Most of the genes with the highest contribution to PC1 are the genes between 500 and 600, which was also the case for the unscaled data.  These are the genese that had different mean and standard deviation to other genes without scaling.  Good to see that get consistent results performing PCA on scaled data.

```{r gene_pca_scaled_1}
pr_rotation <- as_tibble(pr_gene_unscaled$rotation)
pr_rotation$gene <- row.names(pr_gene_unscaled$rotation)
pr_rotation <- pr_rotation %>%
  select(gene, everything()) %>%
  arrange(-abs(PC1))

slice(pr_rotation, 1:100)
```

Plot a few of the genes that have high loadings in PC1 but which are not in range V500:V600.  These genes also split the healthy and diseased tissue samples well.
```{r gene_pc_plot}
ggplot(gene_df, aes(V11, V15, colour = health)) +
  geom_point()

ggplot(gene_df, aes(V16, V13, colour = health)) +
  geom_point()

ggplot(gene_df, aes(V12, V18, colour = health)) +
  geom_point()
```
